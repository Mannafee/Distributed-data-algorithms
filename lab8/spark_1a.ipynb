{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"Spark Count\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark create RDD example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating two list a and b as tuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(\"spark\",), (\"rdd\",), (\"python\",), (\"context\",), (\"create\",), (\"class\",)]\n",
    "b = [(\"operation\",), (\"apache\",), (\"scala\",), (\"lambda\",),(\"parallel\",),(\"partition\",)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating spark dataframe and giving alias name as ta and tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| col_ta|\n",
      "+-------+\n",
      "|  spark|\n",
      "|    rdd|\n",
      "| python|\n",
      "|context|\n",
      "| create|\n",
      "|  class|\n",
      "+-------+\n",
      "\n",
      "+---------+\n",
      "|   col_tb|\n",
      "+---------+\n",
      "|operation|\n",
      "|   apache|\n",
      "|    scala|\n",
      "|   lambda|\n",
      "| parallel|\n",
      "|partition|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tba = spark.createDataFrame(a,['col_ta',])\n",
    "tbb=spark.createDataFrame((b),['col_tb',])\n",
    "ta = tba.alias('ta')\n",
    "tb = tbb.alias('tb')\n",
    "ta.show()\n",
    "tb.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining the table using right outer join\n",
    "- comparing the data of ta and tb to create right outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|col_ta|   col_tb|\n",
      "+------+---------+\n",
      "|  null|operation|\n",
      "|  null|   lambda|\n",
      "|  null|partition|\n",
      "|  null| parallel|\n",
      "|  null|    scala|\n",
      "|  null|   apache|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right_outer_join = ta.join(tb, ta.col_ta == tb.col_tb,how='right_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining the table using Full outer join\n",
    "comparing the data of ta and tb to create full outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "| col_ta|   col_tb|\n",
      "+-------+---------+\n",
      "|   null|operation|\n",
      "|   null|   lambda|\n",
      "|context|     null|\n",
      "|   null|partition|\n",
      "| create|     null|\n",
      "|    rdd|     null|\n",
      "|   null| parallel|\n",
      "|   null|    scala|\n",
      "|   null|   apache|\n",
      "|  spark|     null|\n",
      "|  class|     null|\n",
      "| python|     null|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_outer_join = ta.join(tb, ta.col_ta == tb.col_tb,how='full_outer').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting 's' using map and reduce\n",
    "   The data is parallelized then flatmap is used to map the data and reducebykey function is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a the occurance of s is : PythonRDD[99] at RDD at PythonRDD.scala:53\n",
      "For a the occurance of s is : PythonRDD[105] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "#count \"s\" using map reduce\n",
    "char_counts_a = sc.parallelize(a).flatMap(lambda each: each[0].count('s')).map(lambda char: char).map(lambda c: (c, 1)).reduceByKey(lambda v1, v2: v1 + v2)\n",
    "lis_a = char_counts_a.show()\n",
    "char_counts_b = sc.parallelize(b).flatMap(lambda each: each[0].count('s')).map(lambda char: char).map(lambda c: (c, 1)).reduceByKey(lambda v1, v2: v1 + v2)\n",
    "lis_b = char_counts_b.show()\n",
    "\n",
    "print(\"For a the occurance of s is :\",char_counts_a)\n",
    "print(\"For a the occurance of s is :\",char_counts_b)\n",
    "print(\"For a the occurance of s is :\",lis_a,'****',\"For b the occurance of s is :\",lis_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a the occurance of s is : 3****\n",
    "For b the occurance of s is : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting 'S' using using aggregate function\n",
    " The data is parallelized then agregated using lambda function to count 's' for both a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a the occurance of s is : 3\n",
      "For b the occurance of s is : 1\n"
     ]
    }
   ],
   "source": [
    "#count using aggregate fuction\n",
    "count_a_aggregate=sc.parallelize(a).aggregate(0, lambda i, x: i + x[0].count('s'), lambda i, j: i+j)\n",
    "print(\"For a the occurance of s is :\",count_a_aggregate)\n",
    "count_b_aggregate=sc.parallelize(b).aggregate(0, lambda i, x: i + x[0].count('s'), lambda i, j: i+j)\n",
    "print(\"For b the occurance of s is :\",count_b_aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
